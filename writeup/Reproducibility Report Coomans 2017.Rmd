---
title: "Reproducibility Report for study \"Distinguishing Fast and Slow Processes in Accuracy - Response Time Data\" by Coomans F, Hofman A, Brinkhuis M, van der Maas HLJ, and Maris G"
subtitle: " (2016, PLoS ONE 11(5): e0155149. https://doi.org/10.1371/journal.pone.0155149)"
author: "Reproducibility Project Author: Michael Hardy, hardym@stanford.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction
Coomans et al. explore and compare models that seek to measure the latent variable of problem-solving ability by analyzing speed in solution finding in addition to respondents' accuracy, which is typically the only measure used in Item Response Theory. Their paper evaluates new and commonly used psychometric models when using response-time by explicating how the underlying latent structure is represented in each. With the dramatic increase in availability of participant process data originating from increased use of technology during educational assessments, these methods also shed light into how other assessment process data might be modeled and used to better measure the desired underlying latent traits. The authors analyze item-pairs: two assessment items that seek to measure the same underlying trait. The authors explore these models using empirical data across a variety of cognitive tasks, provide justification for their selection of their recommended model, and supplementally evaluate the implications of using these models by demonstrating through error-analyses to highlight qualitative differences in erroneous responses based on response speed. 

**Key Analyses** The most important reproduction analyses are those of tables 3 and 4, which are used to justify the conclusions of the paper. No additional analyses are planned yet for this paper.

### Justification for choice of study

This study is of interest, as it represents an explicit link between psychometrics and cognitive psychology which are both subjects of interest. The paper is written by some of the most respected psychometricians in the field. While written in 2017, its content is more relevant today than ever, as it compares methods of incorporating process data into understanding latent variables targeted by Item Response Theory (IRT). The study uses data from a variety of domains including arithmetic, language learning, game-playing problems, and chess to demonstrate the generalizability of the models. 

I have no formal training in psychometrics, IRT, nor measurement theory, so this paper represents the opportunity to learn some of these concepts from some of the best researchers in the field using empirical data from multiple sources. I am interested in learning these foundations, and how world-class researchers navigate solving a novel, relevant problem with these tools.

### Anticipated challenges

As mentioned, I have no formal training in the branches of statistics involved, so I anticipate replicating the psychometric procedures may be very challenging for me. 

### Links

Project repository (on Github): https://github.com/psych251/coomans2016

Original paper (as hosted in your repo): https://github.com/psych251/coomans2016/blob/main/original_paper/distinguishing_fast_slow_article.pdf 

## Methods

### Description of the steps required to reproduce the results

Please describe all the steps necessary to reproduce the key result(s) of this study. 

1. Gain access to the data
    a. Get access to oefenweb_nl_app database (where the raw data is stored)
    b. Create a SQL query for collecting the data from the various assessments
    c. Pull data into R
2. Organize the data
    a. Clean and "tidy" the data using tidyverse
    b. Apply the coding described in the paper to classify item relationships, 
    c. identify and create item pairs based on the criteria provided in the paper.
3. Understand the mathematical relationships found in contingency tables for item pairs
4. Reconstruct tables 2 and 5 found in paper and any needed contingency tables using item pairs
5. Understand the mathematical relationships of the four different models
6. Create functions in R that represent the mathematical models describing the items and their relationships, including the mathematical relationships described in tables 6, 7, 8, 9, and 10.
6. Calculate Maximum Likelihood from contingency tables to find estimates for each of the models' parameters using eq. 16, 17, 23, 30
7. Understand the mathematical relationships represented in the tests used and calculate Chi-squared values for the models and reproduce tables 3 and 4
8. Compare results with original paper
9. Repeat steps 4, 5, 7, and 8 for the chess data set, which are slightly different as I understand it.


### Differences from original study

No differences from the original paper are planned. I intend to use the original data, definitions, and their formulas for reproduction.

## Project Progress Check 1

### Measure of success

Measure of success will be comparisons of the data found in their Table 3 and Table 4, since this is what is used to draw their conclusion.


### Pipeline progress

I can now reproduce Table 3 using their code. However, I believe I have found a filtering error on their part, leading them to accidentally throw out most of their data. So I re-ran the code for Table 3, correcting by including all item pairs as they describe, and it changes the results. This however, does NOT change their overall conclusions, but it *would* change the 'Discussion' section of their paper. 

I have included my data preparation and overall results below. 

## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r include=F, warning=F}
### Data Preparation

#### Load Relevant Libraries and Functions
require(rootSolve)
library(tidyr)
library(dplyr)
library(stringr) # useful for some string manipulation
library(ggplot2)
library(data.table)

#### Import data
## Data is found in separate RData files, so the import happens as I compute each iteration of the file.
## data is in addition.RData, subtraction.RData, multiplication.RData, division.RData, set.RData and letterchaos.RData
## Item score data is found in the res_max2 data.frame.  Item pairs are found in the the combpair table.
assessment.list = c("multiplication","division","addition","subtraction","set","letterchaos")
table3.assessment.list = assessment.list[1:4]
model.list = c("CIM","CDM","C2P&3I","C3P&2I")
table3 = data.frame(matrix(ncol=5,nrow=4, dimnames=list(NULL, c("domain", model.list))))
table3$domain = table3.assessment.list


#### Data exclusion / filtering
## The data exclusions/filtering happens with each separate step.
#### Prepare data for analysis - create columns etc.

for (asmt in 1:length(table3.assessment.list)){
  assessment = table3.assessment.list[asmt]
  load(str_c("/Users/mhardy/Library/CloudStorage/OneDrive-Stanford/PSY251/Reproducibility/Fast_slow_processes/osfstorage-archive","/",assessment,".RData"))
  # data is in addition.RData, subtraction.RData, multiplication.RData, division.RData, set.RData and letterchaos.RData
  ## Item score data is found in the res_max2 data.frame.  Item pairs are found in the the combpair table.
  print(paste("Now computing for", assessment))
  
  
  n.item.pairs.list = c(435, 435, 435, 435, 45, 45)
  n.item.pairs=435 # number of item pairs: 435 for addition, subtraction, multiplication and division and 45 for Set and Letter Chaos
  n.item.pairs=ncol(combpair)
  
  cond.dep.par=matrix(NA,n.item.pairs,7)
  cond.dep.X=rep(NA,n.item.pairs)
  cond.ind.par=matrix(NA,n.item.pairs,10)
  cond.ind.X=rep(NA,n.item.pairs)
  m.2P.3I_par=matrix(NA,n.item.pairs,14)
  m.2P.3I_X=matrix(NA,n.item.pairs)
  m.3P.2I.par=matrix(NA,n.item.pairs,14)
  m.3P.2I.X=matrix(NA,n.item.pairs)
  CDM_par=matrix(NA,n.item.pairs,12)
  CDM_X=rep(NA,n.item.pairs)
  
  countpair=0 # count pairs with more than 500 observations
  nosolution=rep(0,n.item.pairs)
  
  for(pair in 1:n.item.pairs){
    #####################################
    # Extract item pair contingency table
    #####################################
    if (pair %% 50 ==0){
      print(paste("Pair",pair,"computing."))
    }
    item.1=combpair[1,pair]
    item.2=combpair[2,pair]
    df.1 = res_max2 %>% filter(item_id == item.1) %>% filter((score != 0)|(response_in_milliseconds == 20000)) %>% arrange(days,user_id, desc(created_UNIX))  %>% distinct(user_id, .keep_all = T)
    df.2 = res_max2 %>% filter(item_id == item.2) %>% filter((score != 0)|(response_in_milliseconds == 20000)) %>% arrange(days,user_id, desc(created_UNIX))  %>% distinct(user_id, .keep_all = T)
    respairqm2 = inner_join(df.1, df.2, by="user_id", suffix=c("_1","_2"))
  
    TMP=floor((1+respairqm2[,c('score_1','score_2')])*2)
    T=table(factor(x=TMP[,1],levels=c(0,1,2,3)),factor(x=TMP[,2],levels=c(0,1,2,3)))
    A=T/sum(T)
    
    ## only keeping item pairs if there are at least 500 total observations
    if(sum(T)>500){
      countpair=countpair+1
      #####################
      # Estimation of CDM
      #####################
      
      pi0=A[1,1] #pi0
      pi12=A[1,2]+A[2,1] #pi12
      pi1=A[1,3]+A[2,2]+A[3,1] #pi1
      pi32=A[1,4]+A[2,3]+A[3,2]+A[4,1] #pi32
      pi2=A[2,4]+A[3,3]+A[4,2] #pi2
      pi52=A[3,4]+A[4,3] #pi52
      
  
      f.cond.dep=function(x){
        pi12*x/(2+2*x)+pi1*(x^2+x/2)/(1+x+x^2)+pi32*(3/2*x^3+x^2+x/2)/(1+x+x^2+x^3)+pi2*(3/2*x^2+x+1/2)/(x^2+x+1)+pi52*(3/2*x+1)/(x+1)+3*A[4,4]/2-sum(A[,2])/2-sum(A[,3])-3*sum(A[,4])/2
      }
  
      cond.dep.par[pair,7]=multiroot(f=f.cond.dep,start=1,positive=TRUE)$root
      x=cond.dep.par[pair,7]
  
      model.cond.dep=c(pi0,pi12/(1+x),pi1/(1+x+x^2),pi32/(1+x+x^2+x^3),pi12*x/(1+x),pi1*x/(1+x+x^2),pi32*x/(1+x+x^2+x^3),pi2/(1+x+x^2),pi1*x^2/(1+x+x^2),pi32*x^2/(1+x+x^2+x^3),pi2*x/(1+x+x^2),pi52/(1+x),pi32*x^3/(1+x+x^2+x^3),pi2*x^2/(1+x+x^2),pi52*x/(1+x))
      N=1-sum(model.cond.dep)
  
      est_TS=matrix(c(model.cond.dep,N),4,4)*sum(T)
  
      if((length(which(est_TS<1))==0)&&(length(which(est_TS>=5)>=13))){cond.dep.X[pair]=sum((est_TS-T)^2/est_TS)}   # no expected frequencies under 1 and at least 13 of 16 cells (~ 80 % of the cells) has an expected frequency that is at least 5
  
      ############################
      # Estimation of CIM
      ############################
  
      pi02=A[1,1] #pi02
      pi01=A[1,2]+A[2,1] #pi01
      pi10=A[2,3]+A[3,2] #pi10
      pi21=A[3,4]+A[4,3] #pi21
      pi11=A[1,3]+A[2,4]+A[3,1]+A[4,2] #pi11
      pi12=A[1,4]+A[4,1] #pi12
      pi00=A[2,2] #pi00
      pi20=A[3,3] #pi20
      
      
      f.cond.ind=function(x){
      c(pi11*(x[1]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi10*x[1]/(1+x[1])+pi12*x[1]/(1+x[1])+pi20+pi21+A[4,4]-sum(A[,3])-sum(A[,4]),
        pi11*(x[2]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi01*x[2]/(1+x[2])+pi21*x[2]/(1+x[2])+pi02+pi12+A[4,4]-sum(A[,1])-sum(A[,4])
      )
      }
  
      root=multiroot(f=f.cond.ind,start=c(1,1),positive=TRUE)$root
  
      alpha1=root[1] #alpha1
      alpha2=root[2] #alpha2
  
      model.cond.ind=c(pi02,pi01*alpha2/(1+alpha2),pi11*alpha2/(1+alpha1+alpha2+alpha1*alpha2),pi12/(1+alpha1),pi01/(1+alpha2),pi00,pi10/(1+alpha1),pi11/(1+alpha1+alpha2+alpha1*alpha2),pi11*alpha1/(1+alpha1+alpha2+alpha1*alpha2),pi10*alpha1/(1+alpha1),pi20,pi21/(1+alpha2),pi12*alpha1/(1+alpha1),pi11*alpha1*alpha2/(1+alpha1+alpha2+alpha1*alpha2),pi21*alpha2/(1+alpha2))
      Z=1-sum(model.cond.ind)
      est_TH=matrix(c(model.cond.ind,Z),4,4)*sum(T)
  
      if((length(which(est_TH<1))==0)&&(length(which(est_TH>=5)>=13))){cond.ind.X[pair]=sum((est_TH-T)^2/est_TH)}
  
      ############################
      # Estimation of 2P&3I
      ############################
      m.2P.3I_par[pair,1]=A[1,1] #phi1
      m.2P.3I_par[pair,2]=A[1,4]+A[4,1] #phi2
      m.2P.3I_par[pair,3]=A[2,2] #phi4
      m.2P.3I_par[pair,4]=A[2,3]+A[3,2] #phi5
      m.2P.3I_par[pair,5]=A[3,3] #phi6
      m.2P.3I_par[pair,6]=A[1,2] #phi7
      m.2P.3I_par[pair,7]=A[1,3]+A[4,2] #phi8
      m.2P.3I_par[pair,8]=A[4,3] #phi9
      m.2P.3I_par[pair,9]=A[2,1] #phi10
      m.2P.3I_par[pair,10]=A[2,4]+A[3,1] #phi11
      m.2P.3I_par[pair,11]=A[3,4] #phi12
  
      f_m.2P.3I=function(x){
        c((A[3,1]+A[4,1])/x[1]-(A[1,4]+A[4,1])/(1+x[1])-(A[2,4]+A[3,1])*x[2]/(x[1]*x[2]+x[3]),(A[3,1]+A[3,2])/x[2]-(A[2,3]+A[3,2])/(1+x[2])-(A[2,4]+A[3,1])*x[1]/(x[1]*x[2]+x[3]),(A[2,4]+A[4,2])/x[3]-(A[1,3]+A[4,2])/(1+x[3])-(A[2,4]+A[3,1])/(x[1]*x[2]+x[3]))
      }
  
      S1=1
      S2=1
      counter=0
      while(((S1>0)|(S2>0))&(counter<500)){ # find proper solution with rootsolve function; can be cross-checked with Maple
        root=multiroot(f=f_m.2P.3I,start=runif(3),positive=TRUE)$root
        S1=sum(root==c(0,0,0))
        S2=sum(root>250)
        counter=counter+1
      }

      if(counter>=500){nosolution[pair]=1}
      if(counter<500){
  
        m.2P.3I_par[pair,12]=root[1] #alpha1
        m.2P.3I_par[pair,13]=root[2] #alpha2
        m.2P.3I_par[pair,14]=root[3] #alpha3
  
        model_m.2P.3I=c(m.2P.3I_par[pair,1],m.2P.3I_par[pair,6],m.2P.3I_par[pair,7]/(1+root[3]),m.2P.3I_par[pair,2]/(1+root[1]),m.2P.3I_par[pair,9],m.2P.3I_par[pair,3],m.2P.3I_par[pair,4]/(1+root[2]),m.2P.3I_par[pair,10]*root[3]/(root[1]*root[2]+root[3]),m.2P.3I_par[pair,10]*root[1]*root[2]/(root[1]*root[2]+root[3]),m.2P.3I_par[pair,4]*root[2]/(1+root[2]),m.2P.3I_par[pair,5],m.2P.3I_par[pair,11],m.2P.3I_par[pair,2]*root[1]/(1+root[1]),m.2P.3I_par[pair,7]*root[3]/(1+root[3]),m.2P.3I_par[pair,8])
        Z=1-sum(model_m.2P.3I)
        est_Tm.2P.3I=matrix(c(model_m.2P.3I,Z),4,4,byrow=TRUE)*sum(T)
        if((length(which(est_Tm.2P.3I<1))==0)&&(length(which(est_Tm.2P.3I>=5)>=13))){m.2P.3I_X[pair]=sum((est_Tm.2P.3I-T)^2/est_Tm.2P.3I)}
      }
  
      ############################
      # Estimation of 3P&2I
      ############################
      m.3P.2I.par[pair,1]=A[1,1] #phi1
      m.3P.2I.par[pair,2]=A[1,4]+A[4,1] #phi2
      m.3P.2I.par[pair,3]=A[2,2] #phi4
      m.3P.2I.par[pair,4]=A[2,3]+A[3,2] #phi5
      m.3P.2I.par[pair,5]=A[3,3] #phi6
      m.3P.2I.par[pair,6]=A[1,2] #phi7
      m.3P.2I.par[pair,7]=A[1,3] #phi8
      m.3P.2I.par[pair,8]=A[4,2] #phi9
      m.3P.2I.par[pair,9]=A[4,3] #phi10
      m.3P.2I.par[pair,10]=A[2,1] #phi11
      m.3P.2I.par[pair,11]=A[2,4] #phi12
      m.3P.2I.par[pair,12]=A[3,1] #phi13
      m.3P.2I.par[pair,13]=A[3,4] #phi14
  
      m.3P.2I.par[pair,14]=(A[3,2]+A[4,1])/(A[1,4]+A[2,3]) #alpha
  
      alpha=m.3P.2I.par[pair,14]
  
      model.m.3P.2I=c(m.3P.2I.par[pair,1],m.3P.2I.par[pair,6],m.3P.2I.par[pair,7],m.3P.2I.par[pair,2]/(1+alpha),m.3P.2I.par[pair,10],m.3P.2I.par[pair,3],m.3P.2I.par[pair,4]/(1+alpha),m.3P.2I.par[pair,11],m.3P.2I.par[pair,12],m.3P.2I.par[pair,4]*alpha/(1+alpha),m.3P.2I.par[pair,5],m.3P.2I.par[pair,13],m.3P.2I.par[pair,2]*alpha/(1+alpha),m.3P.2I.par[pair,8],m.3P.2I.par[pair,9])
      Z=1-sum(model.m.3P.2I)
      est_Tm.3P.2I=matrix(c(model.m.3P.2I,Z),4,4,byrow=TRUE)*sum(T)
      if((length(which(est_Tm.3P.2I<1))==0)&&(length(which(est_Tm.3P.2I>=5)>=13))){m.3P.2I.X[pair]=sum((est_Tm.3P.2I-T)^2/est_Tm.3P.2I)}
  

    }
  }
  
  table3[asmt,"CDM"] = length(which(cond.dep.X>26.12))/sum(!is.na(cond.dep.X))
  table3[asmt,"CIM"] = length(which(cond.ind.X>20.52))/sum(!is.na(cond.ind.X))
  table3[asmt,"C2P.3I"] = length(which(m.2P.3I_X>10.83))/sum(!is.na(m.2P.3I_X))
  table3[asmt,"C3P.2I"] = length(which(m.3P.2I.X>10.83))/sum(!is.na(m.3P.2I.X))
  print(table3)

}

```

```{r}
table3
```


### Key analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

I am able to reproduce their results using their code, but I think I have found an error in their code that reduces their sample of item pairs. 


## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them).  None of these need to be long.
