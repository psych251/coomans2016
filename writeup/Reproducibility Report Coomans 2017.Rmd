---
title: "Reproducibility Report for study \"Distinguishing Fast and Slow Processes in Accuracy - Response Time Data\" by Coomans F, Hofman A, Brinkhuis M, van der Maas HLJ, and Maris G"
subtitle: " (2016, PLoS ONE 11(5): e0155149. https://doi.org/10.1371/journal.pone.0155149)"
author: "Reproducibility Project Author: Michael Hardy, hardym@stanford.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction
Coomans et al. explore and compare models that seek to measure the latent variable of problem-solving ability by analyzing speed in solution finding in addition to respondents' accuracy, which is typically the only measure used in Item Response Theory. Their paper evaluates new and commonly used psychometric models when using response-time by explicating how the underlying latent structure is represented in each. With the dramatic increase in availability of participant process data originating from increased use of technology during educational assessments, these methods also shed light into how other assessment process data might be modeled and used to better measure the desired underlying latent traits. The authors analyze item-pairs: two assessment items that seek to measure the same underlying trait. The authors explore these models using empirical data across a variety of cognitive tasks, provide justification for their selection of their recommended model, and supplementally evaluate the implications of using these models by demonstrating through error-analyses to highlight qualitative differences in erroneous responses based on response speed. 

**Key Analyses** The most important reproduction analyses are those of tables 3 and 4, which are used to justify the conclusions of the paper. No additional analyses are planned yet for this paper.

### Justification for choice of study

This study is of interest, as it represents an explicit link between psychometrics and cognitive psychology which are both subjects of interest. The paper is written by some of the most respected psychometricians in the field. While written in 2017, its content is more relevant today than ever, as it compares methods of incorporating process data into understanding latent variables targeted by Item Response Theory (IRT). The study uses data from a variety of domains including arithmetic, language learning, game-playing problems, and chess to demonstrate the generalizability of the models. 

I have no formal training in psychometrics, IRT, nor measurement theory, so this paper represents the opportunity to learn some of these concepts from some of the best researchers in the field using empirical data from multiple sources. I am interested in learning these foundations, and how world-class researchers navigate solving a novel, relevant problem with these tools.

### Anticipated challenges

As mentioned, I have no formal training in the branches of statistics involved, so I anticipate replicating the psychometric procedures may be very challenging for me. 

### Links

Project repository (on Github): https://github.com/psych251/coomans2016

Original paper (as hosted in your repo): https://github.com/psych251/coomans2016/blob/main/original_paper/distinguishing_fast_slow_article.pdf 

## Methods

### Description of the steps required to reproduce the results

Please describe all the steps necessary to reproduce the key result(s) of this study. 

1. Gain access to the data
    a. Get access to oefenweb_nl_app database (where the raw data is stored)
    b. Create a SQL query for collecting the data from the various assessments
    c. Pull data into R
2. Organize the data
    a. Clean and "tidy" the data using tidyverse
    b. Apply the coding described in the paper to classify item relationships, 
    c. identify and create item pairs based on the criteria provided in the paper.
3. Understand the mathematical relationships found in contingency tables for item pairs
4. Reconstruct tables 2 and 5 found in paper and any needed contingency tables using item pairs
5. Understand the mathematical relationships of the four different models
6. Create functions in R that represent the mathematical models describing the items and their relationships, including the mathematical relationships described in tables 6, 7, 8, 9, and 10.
6. Calculate Maximum Likelihood from contingency tables to find estimates for each of the models' parameters using eq. 16, 17, 23, 30
7. Understand the mathematical relationships represented in the tests used and calculate Chi-squared values for the models and reproduce tables 3 and 4
8. Compare results with original paper
9. Repeat steps 4, 5, 7, and 8 for the chess data set.


### Differences from original study

No differences from the original paper are planned. I intend to use the original data, definitions, and their formulas for reproduction.

## Project Progress Check 1

### Measure of success

Measure of success will be comparisons of the data found in their Table 3 and Table 4, since this is what is used to draw their conclusion.


### Pipeline progress
All pipeline steps are completed.

## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r analysis, results='hide', warning=FALSE, message=FALSE}
### Data Preparation

#### Load Relevant Libraries and Functions
require(rootSolve)
library(tidyr)
library(dplyr)
library(stringr) # useful for some string manipulation
library(ggplot2)
library(data.table)
setwd("~/Library/CloudStorage/OneDrive-Stanford/PSY251/Reproducibility/Fast_slow_processes/osfstorage-archive")
assessment.list = c("multiplication","division","addition","subtraction","set","letterchaos","chess")
table3.assessment.list = assessment.list[1:4]
model.list = c("CIM","CDM","C2P&3I","C3P&2I")
table3 = data.frame(matrix(ncol=5,nrow=4, dimnames=list(NULL, c("domain", model.list))))
table3$domain = table3.assessment.list

table4.assessment.list = assessment.list[5:7]
table4 = data.frame(matrix(ncol=5,nrow=3, dimnames=list(NULL, c("domain", model.list))))
table4$domain = table4.assessment.list

for (asmt in 1:length(assessment.list)){
  assessment = assessment.list[asmt]
  load(str_c(getwd(),"/",assessment,".RData"))
  # data is stored as addition.RData, subtraction.RData, multiplication.RData, division.RData, set.RData and letterchaos.RData
  ## Item score data is found in the res_max2 data.frame.  Item pairs are found in the the combpair table.
  print(paste("Now computing for", assessment))
  n.item.pairs=ncol(combpair)
  
  cond.dep.par=matrix(NA,n.item.pairs,7)
  cond.dep.X=rep(NA,n.item.pairs)
  cond.ind.par=matrix(NA,n.item.pairs,10)
  cond.ind.X=rep(NA,n.item.pairs)
  m.2P.3I_par=matrix(NA,n.item.pairs,14)
  m.2P.3I_X=matrix(NA,n.item.pairs)
  m.3P.2I.par=matrix(NA,n.item.pairs,14)
  m.3P.2I.X=matrix(NA,n.item.pairs)
  
  countpair=0 # count pairs with more than 500 observations
  nosolution=rep(0,n.item.pairs)
  
  for(pair in 1:n.item.pairs){
    #####################################
    # Extract item pair contingency table
    #####################################
    if (pair %% 50 ==0){
      print(paste("Pair",pair,"computing."))
    }
    item.1=combpair[1,pair]
    item.2=combpair[2,pair]

    #####################################
    # This is my code for item pair creation (except chess)
    #####################################      
    df.1 = res_max2 %>% filter(item_id == item.1) %>%
      filter((score != 0)|(response_in_milliseconds == 20000)) %>%
      arrange(days,user_id, created_UNIX) %>%
      distinct(user_id,days, .keep_all = TRUE)
    df.2 = res_max2 %>% filter(item_id == item.2) %>%
      filter((score != 0)|(response_in_milliseconds == 20000)) %>%
      arrange(days,user_id, created_UNIX)  %>%
      distinct(user_id,days, .keep_all = TRUE)
    respairqm2 = inner_join(df.1, df.2, by=c("user_id","days"), suffix=c("_1","_2"))%>%
      arrange(days,user_id, desc(created_UNIX_1)) %>%
      distinct(user_id,.keep_all = TRUE)      
    
    TMP=floor((1+respairqm2[,c('score_1','score_2')])*2)
    dist.table=table(factor(x=TMP[,1],levels=c(0,1,2,3)),factor(x=TMP[,2],levels=c(0,1,2,3)))
    A=dist.table/sum(dist.table)
    
    ## only keeping item pairs if there are at least 500 total observations
    if(sum(dist.table)>500){  
      countpair=countpair+1
      #####################
      # Estimation of CDM
      #####################
      pi0=A[1,1] #pi0
      pi12=A[1,2]+A[2,1] #pi12
      pi1=A[1,3]+A[2,2]+A[3,1] #pi1
      pi32=A[1,4]+A[2,3]+A[3,2]+A[4,1] #pi32
      pi2=A[2,4]+A[3,3]+A[4,2] #pi2
      pi52=A[3,4]+A[4,3] #pi52
      
      f.cond.dep=function(x){
        pi12*x/(2+2*x)+pi1*(x^2+x/2)/(1+x+x^2)+pi32*(3/2*x^3+x^2+x/2)/(1+x+x^2+x^3)+pi2*(3/2*x^2+x+1/2)/(x^2+x+1)+pi52*(3/2*x+1)/(x+1)+3*A[4,4]/2-sum(A[,2])/2-sum(A[,3])-3*sum(A[,4])/2
      }
      x=multiroot(f=f.cond.dep,start=1,positive=TRUE,verbose = F)$root
  
      model.cond.dep=c(pi0,
                       pi12/(1+x),
                       pi1/(1+x+x^2),
                       pi32/(1+x+x^2+x^3),
                       pi12*x/(1+x),
                       pi1*x/(1+x+x^2),
                       pi32*x/(1+x+x^2+x^3),
                       pi2/(1+x+x^2),
                       pi1*x^2/(1+x+x^2),
                       pi32*x^2/(1+x+x^2+x^3),
                       pi2*x/(1+x+x^2),
                       pi52/(1+x),
                       pi32*x^3/(1+x+x^2+x^3),
                       pi2*x^2/(1+x+x^2),
                       pi52*x/(1+x))
      model.cond.dep=c(model.cond.dep,1-sum(model.cond.dep))
      est_TS=matrix(model.cond.dep,4,4)*sum(dist.table)  
      ## Create model contingency table
      if((length(which(est_TS<1))==0)&&(length(which(est_TS>=5)>=13))){cond.dep.X[pair]=sum((est_TS-dist.table)^2/est_TS)}   # no expected frequencies under 1 and at least 13 of 16 cells (~ 80 % of the cells) has an expected frequency that is at least 5
  
      ############################
      # Estimation of CIM
      ############################
      pi02=A[1,1] #pi02
      pi01=A[1,2]+A[2,1] #pi01
      pi10=A[2,3]+A[3,2] #pi10
      pi21=A[3,4]+A[4,3] #pi21
      pi11=A[1,3]+A[2,4]+A[3,1]+A[4,2] #pi11
      pi12=A[1,4]+A[4,1] #pi12
      pi00=A[2,2] #pi00
      pi20=A[3,3] #pi20
      
      f.cond.ind=function(x){
      c(pi11*(x[1]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi10*x[1]/(1+x[1])+pi12*x[1]/(1+x[1])+pi20+pi21+A[4,4]-sum(A[,3])-sum(A[,4]),
        pi11*(x[2]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi01*x[2]/(1+x[2])+pi21*x[2]/(1+x[2])+pi02+pi12+A[4,4]-sum(A[,1])-sum(A[,4])
      )
      }
      root=multiroot(f=f.cond.ind,start=c(1,1),positive=TRUE, verbose = F)$root
      alpha1=root[1] #alpha1
      alpha2=root[2] #alpha2
  
      model.cond.ind=c(pi02,
                       pi01*alpha2/(1+alpha2),
                       pi11*alpha2/(1+alpha1+alpha2+alpha1*alpha2),
                       pi12/(1+alpha1),
                       pi01/(1+alpha2),
                       pi00,
                       pi10/(1+alpha1),
                       pi11/(1+alpha1+alpha2+alpha1*alpha2),
                       pi11*alpha1/(1+alpha1+alpha2+alpha1*alpha2),
                       pi10*alpha1/(1+alpha1),
                       pi20,
                       pi21/(1+alpha2),
                       pi12*alpha1/(1+alpha1),
                       pi11*alpha1*alpha2/(1+alpha1+alpha2+alpha1*alpha2),
                       pi21*alpha2/(1+alpha2))
      
      est_TH=matrix(c(model.cond.ind,1-sum(model.cond.ind)),4,4)*sum(dist.table)  
      ## Create model contingency table
      if((length(which(est_TH<1))==0)&&(length(which(est_TH>=5)>=13))){cond.ind.X[pair]=sum((est_TH-dist.table)^2/est_TH)}
  
      ############################
      # Estimation of 2P&3I
      ############################
      phi1=A[1,1] #phi1
      phi2=A[1,4]+A[4,1] #phi2
      phi4=A[2,2] #phi4
      phi5=A[2,3]+A[3,2] #phi5
      phi6=A[3,3] #phi6
      phi7=A[1,2] #phi7
      phi8=A[1,3]+A[4,2] #phi8
      phi9=A[4,3] #phi9
      phi10=A[2,1] #phi10
      phi11=A[2,4]+A[3,1] #phi11
      phi12=A[3,4] #phi12
      
      f_m.2P.3I=function(x){
        c((A[3,1]+A[4,1])/x[1]-(phi2)/(1+x[1])-(phi11)*x[2]/(x[1]*x[2]+x[3]),
          (A[3,1]+A[3,2])/x[2]-(phi5)/(1+x[2])-(phi11)*x[1]/(x[1]*x[2]+x[3]),
          (A[2,4]+A[4,2])/x[3]-(phi8)/(1+x[3])-(phi11)/(x[1]*x[2]+x[3]))
      }
      S1=1
      S2=1
      counter=0
      while(((S1>0)|(S2>0))&(counter<500)){ 
        root=multiroot(f=f_m.2P.3I,start=runif(3),positive=TRUE, verbose = F)$root
        S1=sum(root==c(0,0,0))
        S2=sum(root>250)
        counter=counter+1
      }
      if(counter>=500){nosolution[pair]=1}
      if(counter<500){
        alpha1=root[1] #alpha1
        alpha2=root[2] #alpha2
        alpha3=root[3] #alpha3
        model_m.2P.3I=c(phi1,
                        phi7,
                        phi8/(1+alpha3),
                        phi2/(1+alpha1),
                        phi10,
                        phi4,
                        phi5/(1+alpha2),
                        phi11*alpha3/(alpha1*alpha2+alpha3),
                        phi11*alpha1*alpha2/(alpha1*alpha2+alpha3),
                        phi5*alpha2/(1+alpha2),
                        phi6,
                        phi12,
                        phi2*alpha1/(1+alpha1),
                        phi8*alpha3/(1+alpha3),
                        phi9)
        est_Tm.2P.3I=matrix(c(model_m.2P.3I,1-sum(model_m.2P.3I)),4,4,byrow=TRUE)*sum(dist.table)
        ## Create model contingency table
        if((length(which(est_Tm.2P.3I<1))==0)&&(length(which(est_Tm.2P.3I>=5)>=13))){m.2P.3I_X[pair]=sum((est_Tm.2P.3I-dist.table)^2/est_Tm.2P.3I)}
      }
      ############################
      # Estimation of 3P&2I
      ############################
      phi1=A[1,1] #phi1
      phi2=A[1,4]+A[4,1] #phi2
      phi4=A[2,2] #phi4
      phi5=A[2,3]+A[3,2] #phi5
      phi6=A[3,3] #phi6
      phi7=A[1,2] #phi7
      phi8=A[1,3] #phi8
      phi9=A[4,2] #phi9
      phi10=A[4,3] #phi10
      phi11=A[2,1] #phi11
      phi12=A[2,4] #phi12
      phi13=A[3,1] #phi13
      phi14=A[3,4] #phi14
      alpha=(A[3,2]+A[4,1])/(A[1,4]+A[2,3]) #alpha
      
      model.m.3P.2I=c(phi1,
                      phi7,
                      phi8,
                      phi2/(1+alpha),
                      phi11,
                      phi4,
                      phi5/(1+alpha),
                      phi12,
                      phi13,
                      phi5*alpha/(1+alpha),
                      phi6,
                      phi14,
                      phi2*alpha/(1+alpha),
                      phi9,
                      phi10)
      model.m.3P.2I=c(model.m.3P.2I,1-sum(model.m.3P.2I))
      est_Tm.3P.2I=matrix(model.m.3P.2I,4,4,byrow=TRUE)*sum(dist.table)
      ## Create model contingency table
      if((length(which(est_Tm.3P.2I<1))==0)&&(length(which(est_Tm.3P.2I>=5)>=13))){m.3P.2I.X[pair]=sum((est_Tm.3P.2I-dist.table)^2/est_Tm.3P.2I)}
    }
  }
  if (asmt < 5){
    table3[asmt,"CDM"] = length(which(cond.dep.X>26.12))/sum(!is.na(cond.dep.X))
    table3[asmt,"CIM"] = length(which(cond.ind.X>20.52))/sum(!is.na(cond.ind.X))
    table3[asmt,"C2P.3I"] = length(which(m.2P.3I_X>10.83))/sum(!is.na(m.2P.3I_X))
    table3[asmt,"C3P.2I"] = length(which(m.3P.2I.X>10.83))/sum(!is.na(m.3P.2I.X))
    print(table3)
  }
  else{
    table4[asmt-4,"CDM"] = length(which(cond.dep.X>26.12))/sum(!is.na(cond.dep.X))
    table4[asmt-4,"CIM"] = length(which(cond.ind.X>20.52))/sum(!is.na(cond.ind.X))
    table4[asmt-4,"C2P.3I"] = length(which(m.2P.3I_X>10.83))/sum(!is.na(m.2P.3I_X))
    table4[asmt-4,"C3P.2I"] = length(which(m.3P.2I.X>10.83))/sum(!is.na(m.3P.2I.X))
    print(table4)    
  }
}

#####################
# CHESS DATA SET
#####################
asmt = 7
assessment = assessment.list[asmt]   
load(str_c(getwd(),"/",assessment,".RData"))
print(paste("Now computing for", assessment))
n.item.pairs=780 # number of item pairs
countpair=0
pair = 1
cond.dep.par=matrix(NA,n.item.pairs,7)
cond.dep.X=rep(NA,n.item.pairs)
cond.ind.par=matrix(NA,n.item.pairs,10)
cond.ind.X=rep(NA,n.item.pairs)
m.2P.3I_par=matrix(NA,n.item.pairs,14)
m.2P.3I_X=matrix(NA,n.item.pairs)
m.3P.2I.par=matrix(NA,n.item.pairs,14)
m.3P.2I.X=matrix(NA,n.item.pairs)

countpair=0 # count pairs with more than 500 observations
nosolution=rep(0,n.item.pairs)
for (r in 2:40){
  for (c in 1:(r-1)){
    #####################
    # Extract item pair contingency table for chess
    #####################
    dist.table=table(factor(x=Y[,r],levels=c(0,0.5,1,1.5)),factor(x=Y[,c],levels=c(0,0.5,1,1.5)))
    A=dist.table/sum(dist.table)
    countpair=countpair+1
    #####################
    # Estimation of CDM
    #####################
    pi0=A[1,1] #pi0
    pi12=A[1,2]+A[2,1] #pi12
    pi1=A[1,3]+A[2,2]+A[3,1] #pi1
    pi32=A[1,4]+A[2,3]+A[3,2]+A[4,1] #pi32
    pi2=A[2,4]+A[3,3]+A[4,2] #pi2
    pi52=A[3,4]+A[4,3] #pi52
    
    f.cond.dep=function(x){
      pi12*x/(2+2*x)+pi1*(x^2+x/2)/(1+x+x^2)+pi32*(3/2*x^3+x^2+x/2)/(1+x+x^2+x^3)+pi2*(3/2*x^2+x+1/2)/(x^2+x+1)+pi52*(3/2*x+1)/(x+1)+3*A[4,4]/2-sum(A[,2])/2-sum(A[,3])-3*sum(A[,4])/2
    }
    x=multiroot(f=f.cond.dep,start=1,positive=TRUE,verbose = F)$root
    
    model.cond.dep=c(pi0,
                     pi12/(1+x),
                     pi1/(1+x+x^2),
                     pi32/(1+x+x^2+x^3),
                     pi12*x/(1+x),
                     pi1*x/(1+x+x^2),
                     pi32*x/(1+x+x^2+x^3),
                     pi2/(1+x+x^2),
                     pi1*x^2/(1+x+x^2),
                     pi32*x^2/(1+x+x^2+x^3),
                     pi2*x/(1+x+x^2),
                     pi52/(1+x),
                     pi32*x^3/(1+x+x^2+x^3),
                     pi2*x^2/(1+x+x^2),
                     pi52*x/(1+x))
    model.cond.dep=c(model.cond.dep,1-sum(model.cond.dep))
    est_TS=matrix(model.cond.dep,4,4)*sum(dist.table)  
    ## Create model contingency table
    if((length(which(est_TS<1))==0)&&(length(which(est_TS>=5)>=13))){cond.dep.X[pair]=sum((est_TS-dist.table)^2/est_TS)}  
    ############################
    # Estimation of CIM
    ############################
    pi02=A[1,1] #pi02
    pi01=A[1,2]+A[2,1] #pi01
    pi10=A[2,3]+A[3,2] #pi10
    pi21=A[3,4]+A[4,3] #pi21
    pi11=A[1,3]+A[2,4]+A[3,1]+A[4,2] #pi11
    pi12=A[1,4]+A[4,1] #pi12
    pi00=A[2,2] #pi00
    pi20=A[3,3] #pi20
    
    f.cond.ind=function(x){
      c(pi11*(x[1]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi10*x[1]/(1+x[1])+pi12*x[1]/(1+x[1])+pi20+pi21+A[4,4]-sum(A[,3])-sum(A[,4]),
        pi11*(x[2]+x[1]*x[2])/(1+x[1]+x[2]+x[1]*x[2])+pi01*x[2]/(1+x[2])+pi21*x[2]/(1+x[2])+pi02+pi12+A[4,4]-sum(A[,1])-sum(A[,4]))
    }
    root=multiroot(f=f.cond.ind,start=c(1,1),positive=TRUE, verbose = F)$root

    alpha1=root[1] #alpha1
    alpha2=root[2] #alpha2
    
    model.cond.ind=c(pi02,
                     pi01*alpha2/(1+alpha2),
                     pi11*alpha2/(1+alpha1+alpha2+alpha1*alpha2),
                     pi12/(1+alpha1),
                     pi01/(1+alpha2),
                     pi00,
                     pi10/(1+alpha1),
                     pi11/(1+alpha1+alpha2+alpha1*alpha2),
                     pi11*alpha1/(1+alpha1+alpha2+alpha1*alpha2),
                     pi10*alpha1/(1+alpha1),
                     pi20,
                     pi21/(1+alpha2),
                     pi12*alpha1/(1+alpha1),
                     pi11*alpha1*alpha2/(1+alpha1+alpha2+alpha1*alpha2),
                     pi21*alpha2/(1+alpha2))
    
    est_TH=matrix(c(model.cond.ind,1-sum(model.cond.ind)),4,4)*sum(dist.table)  
    ## Create model contingency table
    if((length(which(est_TH<1))==0)&&(length(which(est_TH>=5)>=13))){cond.ind.X[pair]=sum((est_TH-dist.table)^2/est_TH)}
    ############################
    # Estimation of 2P&3I
    ############################
    phi1=A[1,1] #phi1
    phi2=A[1,4]+A[4,1] #phi2
    phi4=A[2,2] #phi4
    phi5=A[2,3]+A[3,2] #phi5
    phi6=A[3,3] #phi6
    phi7=A[1,2] #phi7
    phi8=A[1,3]+A[4,2] #phi8
    phi9=A[4,3] #phi9
    phi10=A[2,1] #phi10
    phi11=A[2,4]+A[3,1] #phi11
    phi12=A[3,4] #phi12
    
    f_m.2P.3I=function(x){
      c((A[3,1]+A[4,1])/x[1]-(phi2)/(1+x[1])-(phi11)*x[2]/(x[1]*x[2]+x[3]),
        (A[3,1]+A[3,2])/x[2]-(phi5)/(1+x[2])-(phi11)*x[1]/(x[1]*x[2]+x[3]),
        (A[2,4]+A[4,2])/x[3]-(phi8)/(1+x[3])-(phi11)/(x[1]*x[2]+x[3]))
    }
    
    S1=1
    S2=1
    counter=0
    while(((S1>0)|(S2>0))&(counter<500)){ 
      root=multiroot(f=f_m.2P.3I,start=runif(3),positive=TRUE, verbose = F)$root
      S1=sum(root==c(0,0,0))
      S2=sum(root>250)
      counter=counter+1
    }
    if(counter>=500){nosolution[pair]=1}
    if(counter<500){
      alpha1=root[1] #alpha1
      alpha2=root[2] #alpha2
      alpha3=root[3] #alpha3
      model_m.2P.3I=c(phi1,
                      phi7,
                      phi8/(1+alpha3),
                      phi2/(1+alpha1),
                      phi10,
                      phi4,
                      phi5/(1+alpha2),
                      phi11*alpha3/(alpha1*alpha2+alpha3),
                      phi11*alpha1*alpha2/(alpha1*alpha2+alpha3),
                      phi5*alpha2/(1+alpha2),
                      phi6,
                      phi12,
                      phi2*alpha1/(1+alpha1),
                      phi8*alpha3/(1+alpha3),
                      phi9)
      est_Tm.2P.3I=matrix(c(model_m.2P.3I,1-sum(model_m.2P.3I)),4,4,byrow=TRUE)*sum(dist.table)
      if((length(which(est_Tm.2P.3I<1))==0)&&(length(which(est_Tm.2P.3I>=5)>=13))){m.2P.3I_X[pair]=sum((est_Tm.2P.3I-dist.table)^2/est_Tm.2P.3I)}
    }
    
    ############################
    # Estimation of 3P&2I
    ############################
    phi1=A[1,1] #phi1
    phi2=A[1,4]+A[4,1] #phi2
    phi4=A[2,2] #phi4
    phi5=A[2,3]+A[3,2] #phi5
    phi6=A[3,3] #phi6
    phi7=A[1,2] #phi7
    phi8=A[1,3] #phi8
    phi9=A[4,2] #phi9
    phi10=A[4,3] #phi10
    phi11=A[2,1] #phi11
    phi12=A[2,4] #phi12
    phi13=A[3,1] #phi13
    phi14=A[3,4] #phi14
    alpha=(A[3,2]+A[4,1])/(A[1,4]+A[2,3]) #alpha
    
    model.m.3P.2I=c(phi1,
                    phi7,
                    phi8,
                    phi2/(1+alpha),
                    phi11,
                    phi4,
                    phi5/(1+alpha),
                    phi12,
                    phi13,
                    phi5*alpha/(1+alpha),
                    phi6,
                    phi14,
                    phi2*alpha/(1+alpha),
                    phi9,
                    phi10)
    model.m.3P.2I=c(model.m.3P.2I,1-sum(model.m.3P.2I))
    est_Tm.3P.2I=matrix(model.m.3P.2I,4,4,byrow=TRUE)*sum(dist.table)
    if((length(which(est_Tm.3P.2I<1))==0)&&(length(which(est_Tm.3P.2I>=5)>=13))){m.3P.2I.X[pair]=sum((est_Tm.3P.2I-dist.table)^2/est_Tm.3P.2I)}
    pair=pair+1
  }
}
table4[asmt-4,"CDM"] = length(which(cond.dep.X>26.12))/sum(!is.na(cond.dep.X))
table4[asmt-4,"CIM"] = length(which(cond.ind.X>20.52))/sum(!is.na(cond.ind.X))
table4[asmt-4,"C2P.3I"] = length(which(m.2P.3I_X>10.83))/sum(!is.na(m.2P.3I_X))
table4[asmt-4,"C3P.2I"] = length(which(m.3P.2I.X>10.83))/sum(!is.na(m.3P.2I.X))
print(table4)    

```

```{r}
table3
```

```{r}
table4
```
```{r}
# options(knitr.table.format = function() {
#   if (knitr::is_latex_output())
#     "latex" else "pipe"
# })
knitr::kable(table3)
```
```{r}
# options(knitr.table.format = function() {
#   if (knitr::is_latex_output())
#     "latex" else "pipe"
# })
knitr::kable(table4)
```

### Key analysis

The analyses as specified in the analysis plan.  

I believe I have found a filtering error on their part, leading them to accidentally throw out most of their data. So I re-ran the code for Table 3, correcting by including all item pairs as they describe, and it changes the results. This however, does NOT change their overall conclusions, but it *would* change the 'Discussion' section of their paper. 

I have included my data preparation and overall results below. Note: the results shown do not represent this adjustment; the data reported below should reflect and be identical to their results tables (Table 3 and Table 4). These tables show the percentage of all the contingency tables across all the models that fall outside of their p-value of 0.001. Each number in every table entry indicates what percentage of this number of remaining item pairs has a $\chi^2$-value that exceeds the p = 0.001 threshold for the corresponding number of degrees of freedom (meaning, of all the thousands of tests run, what percent of these models fail to reject the null hypothesis).



###Exploratory analyses

I am able to reproduce their results using their code, but I think I have found an error in their code that reduces their sample of item pairs. 

In their code for item pair creation (except chess), they remove the top and bottom observations so they can use a "sort-->concatenate" matching method, where they sort all observations by day, then by user, then by item, then by a "UNIX" variable (that serves to select between responses by an individual if they did an item more than once on the same day). They take this full matrix, copy it and sort the copy, and then remove the top observation from the original and the bottom of the copy before concatenating them and finally looking for matches. Their method has the effect of matching the lowest UNIX number of the second item in the pair with the lowest in the first. It would eliminate time where a participant did both items in the pair on another day. I think this is an oversight. If both items are done on another occasion, the model should still be valid. Additionally, the removal of the top value and bottom value does occasionally remove a data point in some of the data pairs. 

The chess set comes formatted differently than the other data, and, if this filtering issue was a problem, it was done before the publishing of the data.

## Discussion

### Summary of Reproduction Attempt
I was successfully able to reproduce the models and the data tables, however it took me a while to figure out why instances were slightly different (such as the removal of the top and bottom observations when constructing the item pairs) when I was comparing my code outputs to theirs.

I was able to reproduce their models, and table with my code and check it against theirs. My numbers, if I use their filtering, match theirs perfectly.

### Commentary
I will have one of my advisors review my code and confirm my observations about the matching algorithm they use before contacting them, as he actually knows at least some of the authors.


